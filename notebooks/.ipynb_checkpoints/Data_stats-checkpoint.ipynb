{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dutch-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "#External imports\n",
    "import glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import cv2\n",
    "import pandas as pd\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib import colors\n",
    "from PIL import Image\n",
    "from skimage.io import imread\n",
    "from matplotlib import pyplot as plt  \n",
    "from skimage.transform import rotate, rescale, resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "standing-hanging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of files:  976985\n"
     ]
    }
   ],
   "source": [
    "# Get all the images present in the ss_data drive. using GLOB\n",
    "data_set_path = r'E:\\ss_data\\snapshotserengeti-unzipped\\snapshotserengeti-unzipped\\S9'\n",
    "image_files = glob.glob(os.path.join(data_set_path, \"*/*/*.JPG\"))\n",
    "number_of_files = len(image_files)\n",
    "print(\"No of files: \", number_of_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "verbal-freeze",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of csv files v1: 699931\n",
      "Number of csv files v2: 286211\n",
      "Train df v2: \n",
      "                              file_path  species\n",
      "0  S9/M10/M10_R2/S9_M10_R2_IMAG0041.JPG        2\n",
      "1  S9/M10/M10_R2/S9_M10_R2_IMAG0627.JPG        0\n",
      "2  S9/M10/M10_R2/S9_M10_R2_IMAG0628.JPG        0\n",
      "3  S9/M10/M10_R2/S9_M10_R2_IMAG0629.JPG        0\n",
      "4  S9/M10/M10_R2/S9_M10_R2_IMAG6529.JPG        0\n",
      "Total number of samples: 986142\n"
     ]
    }
   ],
   "source": [
    "# Path prefix to be used for the image_names in the csvs\n",
    "path_prefix = r'E:\\ss_data\\snapshotserengeti-unzipped\\snapshotserengeti-unzipped'\n",
    "# Read the latest train csv \n",
    "train_csv = r'E:\\ss_data\\train_phase2_v6_fixed.csv'\n",
    "train_df = pd.read_csv(train_csv)\n",
    "print(f\"Number of csv files v1: {len(train_df)}\")\n",
    "# Append the other csv files as well \n",
    "#v6 = 000000 to 700000\n",
    "#v7 = 700000 to end\n",
    "train_csv_v2 = r'E:\\ss_data\\train_phase2_v7_fixed.csv'\n",
    "train_df_v2 = pd.read_csv(train_csv_v2)\n",
    "train_df_v2.columns = ['file_path', 'species']\n",
    "print(f\"Number of csv files v2: {len(train_df_v2)}\")\n",
    "print(f'Train df v2: \\n{train_df_v2.head()}')\n",
    "print(f\"Total number of samples: {len(train_df_v2)+len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wound-objective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of csv files: 986142\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.concat([train_df, train_df_v2], axis=0, ignore_index=True)\n",
    "print(f\"Number of csv files: {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "settled-conference",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique species labels in our dataset: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52]\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the unique species in our datasets\n",
    "print(f\"Unique species labels in our dataset: {train_df.species.unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "computational-medicare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of species: 53\n",
      "       species  encoded_species\n",
      "0     aardvark               33\n",
      "1     aardwolf               22\n",
      "2       baboon                4\n",
      "3  batEaredFox               17\n",
      "4        blank                0\n"
     ]
    }
   ],
   "source": [
    "# Read csv for labels enumeration\n",
    "species_enumeration = r'E:\\ss_data\\label_to_species_V3.csv'\n",
    "enum_species_df = pd.read_csv(species_enumeration)\n",
    "print(f\"Number of species: {len(enum_species_df)}\")\n",
    "print(f'{enum_species_df.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "front-double",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train df columns: Index(['file_path', 'species'], dtype='object')\n",
      "Species enum columns: Index(['species', 'encoded_species'], dtype='object')\n",
      "Merged train df: \n",
      "                          file_path_rel species  encoded_species\n",
      "0  S9/B03/B03_R1/S9_B03_R1_IMAG0001.JPG   blank                0\n",
      "1  S9/B03/B03_R1/S9_B03_R1_IMAG0002.JPG   blank                0\n",
      "2  S9/B03/B03_R1/S9_B03_R1_IMAG0015.JPG   blank                0\n",
      "3  S9/B03/B03_R1/S9_B03_R1_IMAG0016.JPG   blank                0\n",
      "4  S9/B03/B03_R1/S9_B03_R1_IMAG0017.JPG   blank                0\n"
     ]
    }
   ],
   "source": [
    "# Join species with the train csv to get the correct labels\n",
    "# Identify joining columns\n",
    "print(f\"Train df columns: {train_df.columns}\")\n",
    "print(f\"Species enum columns: {enum_species_df.columns}\")\n",
    "# join on species and encoded_species\n",
    "train_df = pd.merge(left=train_df, right=enum_species_df, how='inner', left_on='species', right_on='encoded_species')\n",
    "train_df.drop('species_x', axis=1, inplace=True)\n",
    "train_df.columns = ['file_path_rel', 'species', 'encoded_species']\n",
    "print(f'Merged train df: \\n{train_df.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "special-connecticut",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Species only train df: \n",
      "                               file_path_rel     species  encoded_species\n",
      "773884  S9/B03/B03_R1/S9_B03_R1_IMAG0250.JPG  wildebeest                1\n",
      "773885  S9/B03/B03_R1/S9_B03_R1_IMAG0251.JPG  wildebeest                1\n",
      "773886  S9/B03/B03_R1/S9_B03_R1_IMAG0252.JPG  wildebeest                1\n",
      "773887  S9/B03/B03_R1/S9_B03_R1_IMAG0256.JPG  wildebeest                1\n",
      "773888  S9/B03/B03_R1/S9_B03_R1_IMAG0258.JPG  wildebeest                1\n",
      "Species only size: \n",
      "212258\n"
     ]
    }
   ],
   "source": [
    "# Filter on the dataset with species only\n",
    "species_train_df = train_df.loc[(train_df.encoded_species > 0)]\n",
    "print(f'Species only train df: \\n{species_train_df.head()}')\n",
    "print(f'Species only size: \\n{len(species_train_df)}')\n",
    "# Store this as a csv\n",
    "species_train_path = r'E:\\ss_data\\train_species_only.csv'\n",
    "species_train_df.to_csv(species_train_path, sep=',', header=True, index=False, columns=['file_path_rel', 'encoded_species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cutting-excitement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated files : \n",
      "                               file_path_rel species  encoded_species\n",
      "840670  S9/B03/B03_R1/S9_B03_R1_IMAG0057.JPG   zebra                2\n",
      "840671  S9/B03/B03_R1/S9_B03_R1_IMAG0058.JPG   zebra                2\n",
      "840672  S9/B03/B03_R1/S9_B03_R1_IMAG0059.JPG   zebra                2\n",
      "840685  S9/B03/B03_R1/S9_B03_R1_IMAG0911.JPG   zebra                2\n",
      "840727  S9/B03/B03_R2/S9_B03_R2_IMAG0227.JPG   zebra                2\n",
      "Duplicated files length: \n",
      "9188\n",
      "Unique files: \n",
      "9092\n",
      "Train csv df with only one species: \n",
      "                               file_path_rel     species  encoded_species\n",
      "773884  S9/B03/B03_R1/S9_B03_R1_IMAG0250.JPG  wildebeest                1\n",
      "773885  S9/B03/B03_R1/S9_B03_R1_IMAG0251.JPG  wildebeest                1\n",
      "773886  S9/B03/B03_R1/S9_B03_R1_IMAG0252.JPG  wildebeest                1\n",
      "773887  S9/B03/B03_R1/S9_B03_R1_IMAG0256.JPG  wildebeest                1\n",
      "773888  S9/B03/B03_R1/S9_B03_R1_IMAG0258.JPG  wildebeest                1\n",
      "Number of files with one species: \n",
      "193978\n"
     ]
    }
   ],
   "source": [
    "#Extract a df that contains only 1 species\n",
    "duplicatedspecies_df = species_train_df[species_train_df.duplicated(['file_path_rel'])]\n",
    "print(f'Duplicated files : \\n{duplicatedspecies_df.head()}')\n",
    "print(f'Duplicated files length: \\n{len(duplicatedspecies_df)}')\n",
    "print(f'Unique files: \\n{duplicatedspecies_df.file_path_rel.nunique()}')\n",
    "# We see that this list still contains double capture_id's meaning that some images (7128-7065= 63) contain more than 2 species \n",
    "# # cond1 = files_with_multiple_species Remove them from our train csv\n",
    "cond1 = species_train_df['file_path_rel'].isin(duplicatedspecies_df['file_path_rel'])\n",
    "# for the rows where condition = true -> row is dropped cause it refers to image containing multiple species \n",
    "single_species_df = species_train_df.drop(species_train_df[cond1].index)\n",
    "print(f'Train csv df with only one species: \\n{single_species_df.head()}')\n",
    "print(f'Number of files with one species: \\n{len(single_species_df)}')\n",
    "# Store this as a csv\n",
    "single_species_train_path = r'E:\\ss_data\\train_species_only_singles.csv'\n",
    "single_species_df.to_csv(single_species_train_path, sep=',', header=True, index=False, columns=['file_path_rel', 'encoded_species'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "current-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['file_path_local'] = train_(df.file_path.map(lambda x: os.path.join(path_prefix, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "organic-steel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "Removing E:\\ss_data\\snapshotserengeti-unzipped\\snapshotserengeti-unzipped\\S9/S10/S10_R2/S9_S10_R2_IMAG1045.JPG\n",
      "Removing E:\\ss_data\\snapshotserengeti-unzipped\\snapshotserengeti-unzipped\\S9/S10/S10_R2/S9_S10_R2_IMAG1689.JPG\n",
      "Removing E:\\ss_data\\snapshotserengeti-unzipped\\snapshotserengeti-unzipped\\S9/S10/S10_R2/S9_S10_R2_IMAG0216.JPG\n",
      "Removing E:\\ss_data\\snapshotserengeti-unzipped\\snapshotserengeti-unzipped\\S9/S10/S10_R2/S9_S10_R2_IMAG2383.JPG\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "Lengths: RelativePath  = 286211.  species_found  = 286211.  encoded_species  = 286211\n",
      "Filtered Dataframe size 286211\n"
     ]
    }
   ],
   "source": [
    "# train_df needed as input having a file path local, file path rel and a label\n",
    "# Code to resize images and then store then as a train csv ready to be used by the model\n",
    "base_dir = os.path.join(\"e:/\", \"ss_data\")\n",
    "rel_path = []\n",
    "species_found = []\n",
    "encoded_species = []\n",
    "start_from = 700000\n",
    "end_at = 1000000\n",
    "for index, file_name in enumerate(train_df.file_path_local[start_from:]):\n",
    "#     print(file_name)\n",
    "#     print(index)\n",
    "#     print(train_df.file_path[index])\n",
    "#     print(train_df.file_path_local[index])\n",
    "#     print(train_df.encoded_species[index])\n",
    "\n",
    "    index = start_from + index\n",
    "    if index == (start_from+end_at):\n",
    "        print(f'Breaking at size {index}')\n",
    "        break\n",
    "    \n",
    "    image = cv2.imread(file_name)\n",
    "    if image is not None:\n",
    "#         print(image.shape)\n",
    "        image = cv2.resize(image, (256, 256)) \n",
    "#         print(image.shape)\n",
    "        cv2.imwrite(file_name, image)\n",
    "        rel_path.append(train_df.file_path[index])\n",
    "        species_found.append(0 if train_df.encoded_species[index] == 0 else 1)\n",
    "        encoded_species.append(train_df.encoded_species[index])\n",
    "    else:        \n",
    "        if os.path.isfile(file_name):\n",
    "            print(f'Removing {file_name}')\n",
    "            try:\n",
    "                os.remove(os.path.join(file_name))\n",
    "            except:\n",
    "                continue\n",
    "        continue\n",
    "    if index % 10000 == 0:\n",
    "        print(index)    \n",
    "    if index % 100000 == 0:\n",
    "        print(index)\n",
    "        \n",
    "print(f'Lengths: RelativePath  = {len(rel_path)}.  species_found  = {len(species_found)}.  encoded_species  = {len(encoded_species)}')\n",
    "train_df_filtered = pd.DataFrame(np.column_stack([rel_path, species_found, encoded_species]), \n",
    "                               columns=['rel_path', 'boolean_species', 'encoded_species'])\n",
    "print(f'Filtered Dataframe size {len(train_df_filtered)}')\n",
    "output_filepath = os.path.join(base_dir, \"train_phase1_v7.csv\")\n",
    "train_df_filtered.to_csv(output_filepath, columns=['rel_path', 'boolean_species'], sep=',', index=False)\n",
    "\n",
    "output_filepath = os.path.join(base_dir, \"train_phase2_v7.csv\")\n",
    "train_df_filtered.to_csv(output_filepath, columns=['rel_path', 'encoded_species'], sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
